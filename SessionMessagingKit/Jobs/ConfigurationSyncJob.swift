// Copyright Â© 2022 Rangeproof Pty Ltd. All rights reserved.

import Foundation
import Combine
import GRDB
import SessionSnodeKit
import SessionUtilitiesKit

// MARK: - Log.Category

private extension Log.Category {
    static let cat: Log.Category = .create("ConfigurationSyncJob", defaultLevel: .info)
}

// MARK: - ConfigurationSyncJob

public enum ConfigurationSyncJob: JobExecutor {
    public static let maxFailureCount: Int = -1
    public static let requiresThreadId: Bool = true
    public static let requiresInteractionId: Bool = false
    private static let maxRunFrequency: TimeInterval = 3
    private static let waitTimeForExpirationUpdate: TimeInterval = 1
    
    public static func run(_ job: Job, using dependencies: Dependencies) async throws -> JobExecutionResult {
        guard !dependencies[cache: .libSession].isEmpty else {
            return .success(job, stop: true)
        }
        
        /// It's possible for multiple ConfigSyncJob's with the same target (user/group) to try to run at the same time since as soon as
        /// one is started we will enqueue a second one, rather than adding dependencies between the jobs we just continue to defer
        /// the subsequent job while the first one is running in order to prevent multiple configurationSync jobs with the same target
        /// from running at the same time
        ///
        /// **Note:** The one exception to this rule is when the job has `AdditionalTransientData` because if we don't
        /// run it immediately then the `AdditionalTransientData` may not get run at all
        let hasExistingJob: Bool = await dependencies[singleton: .jobRunner]
            .jobInfoFor(
                state: .running,
                filters: JobRunner.Filters(
                    include: [.variant(.configurationSync)],
                    exclude: [
                        job.id.map { .jobId($0) },          /// Exclude this job
                        job.threadId.map { .threadId($0) }  /// Exclude jobs for different config stores
                    ].compactMap { $0 }
                )
            )
            .isEmpty
        
        guard job.transientData != nil || hasExistingJob else {
            /// Defer the job to run `maxRunFrequency` from when this one ran (if we don't it'll try start it again immediately which
            /// is pointless)
            let updatedJob: Job? = dependencies[singleton: .storage].write { db in
                try job
                    .with(nextRunTimestamp: dependencies.dateNow.timeIntervalSince1970 + maxRunFrequency)
                    .upserted(db)
            }
            
            Log.info(.cat, "For \(job.threadId ?? "UnknownId") deferred due to in progress job")
            return .deferred(updatedJob ?? job)
        }
        
        /// If we don't have a userKeyPair yet then there is no need to sync the configuration as the user doesn't exist yet (this will get
        /// triggered on the first launch of a fresh install due to the migrations getting run)
        guard
            let swarmPublicKey: String = job.threadId,
            let pendingPushes: LibSession.PendingPushes = try? dependencies.mutate(cache: .libSession, {
                try $0.pendingPushes(swarmPublicKey: swarmPublicKey)
            })
        else {
            Log.info(.cat, "For \(job.threadId ?? "UnknownId") failed due to invalid data")
            throw StorageError.generic
        }
        
        /// If there is no `pushData` or additional sequence requests then the job can just complete (next time something is updated
        /// we want to try and run immediately so don't scuedule another run in this case)
        guard
            !pendingPushes.pushData.isEmpty ||
            job.transientData != nil
        else {
            Log.info(.cat, "For \(swarmPublicKey) completed with no pending changes")
            ConfigurationSyncJob.startJobsWaitingOnConfigSync(swarmPublicKey, using: dependencies)
            return .success(job, stop: true)
        }
        
        let jobStartTimestamp: TimeInterval = dependencies.dateNow.timeIntervalSince1970
        let messageSendTimestamp: Int64 = dependencies[cache: .snodeAPI].currentOffsetTimestampMs()
        let additionalTransientData: AdditionalTransientData? = (job.transientData as? AdditionalTransientData)
        Log.info(.cat, "For \(swarmPublicKey) started with changes: \(pendingPushes.pushData.count), old hashes: \(pendingPushes.obsoleteHashes.count)")
        
        let authMethod: AuthenticationMethod = try await dependencies[singleton: .storage].readAsync { db in
            try Authentication.with(db, swarmPublicKey: swarmPublicKey, using: dependencies)
        }
        
        let request: Network.PreparedRequest<Network.BatchResponse> = try SnodeAPI.preparedSequence(
            requests: []
                .appending(contentsOf: additionalTransientData?.beforeSequenceRequests)
                .appending(
                    contentsOf: try pendingPushes.pushData
                        .flatMap { pushData -> [ErasedPreparedRequest] in
                            try pushData.data.map { data -> ErasedPreparedRequest in
                                try SnodeAPI
                                    .preparedSendMessage(
                                        message: SnodeMessage(
                                            recipient: swarmPublicKey,
                                            data: data,
                                            ttl: pushData.variant.ttl,
                                            timestampMs: UInt64(messageSendTimestamp)
                                        ),
                                        in: pushData.variant.namespace,
                                        authMethod: authMethod,
                                        using: dependencies
                                    )
                            }
                    }
                )
                .appending(try {
                    guard !pendingPushes.obsoleteHashes.isEmpty else { return nil }
                    
                    return try SnodeAPI.preparedDeleteMessages(
                        serverHashes: Array(pendingPushes.obsoleteHashes),
                        requireSuccessfulDeletion: false,
                        authMethod: authMethod,
                        using: dependencies
                    )
                }())
                .appending(contentsOf: additionalTransientData?.afterSequenceRequests),
            requireAllBatchResponses: (additionalTransientData?.requireAllBatchResponses == true),
            swarmPublicKey: swarmPublicKey,
            snodeRetrievalRetryCount: 0,    // This job has it's own retry mechanism
            requestAndPathBuildTimeout: Network.defaultTimeout,
            using: dependencies
        )
        
        // FIXME: Refactor this to use async/await
        let publisher = request
            .send(using: dependencies)
            .tryMap { (_: ResponseInfoType, response: Network.BatchResponse) -> [ConfigDump] in
                /// The number of responses returned might not match the number of changes sent but they will be returned
                /// in the same order, this means we can just `zip` the two arrays as it will take the smaller of the two and
                /// correctly align the response to the change (we need to manually remove any `beforeSequenceRequests`
                /// results through)
                let responseWithoutBeforeRequests = Array(response
                    .suffix(from: additionalTransientData?.beforeSequenceRequests.count ?? 0))
                
                /// If we `requireAllRequestsSucceed` then ensure every request returned a 2XX status code and
                /// was successfully parsed
                guard
                    additionalTransientData == nil ||
                    additionalTransientData?.requireAllRequestsSucceed == false ||
                    !response
                        .map({ $0 as? ErasedBatchSubResponse })
                        .contains(where: { response -> Bool in
                            (response?.code ?? 0) < 200 ||
                            (response?.code ?? 0) > 299 ||
                            response?.failedToParseBody == true
                        })
                else { throw NetworkError.invalidResponse }
                
                let results: [(pushData: LibSession.PendingPushes.PushData, hash: String?)] = zip(responseWithoutBeforeRequests, pendingPushes.pushData)
                    .map { (subResponse: Any, pushData: LibSession.PendingPushes.PushData) in
                        /// If the request wasn't successful then just ignore it (the next time we sync this config we will try
                        /// to send the changes again)
                        guard
                            let typedResponse: Network.BatchSubResponse<SendMessagesResponse> = (subResponse as? Network.BatchSubResponse<SendMessagesResponse>),
                            200...299 ~= typedResponse.code,
                            !typedResponse.failedToParseBody,
                            let sendMessageResponse: SendMessagesResponse = typedResponse.body
                        else { return (pushData, nil) }
                        
                        return (pushData, sendMessageResponse.hash)
                    }
                
                /// Since this change was successful we need to mark it as pushed and generate any config dumps
                /// which need to be stored
                return try dependencies.mutate(cache: .libSession) { cache in
                    try cache.createDumpMarkingAsPushed(
                        data: results,
                        sentTimestamp: messageSendTimestamp,
                        swarmPublicKey: swarmPublicKey
                    )
                }
            }
        
            do {
                let configDumps: [ConfigDump] = (try await publisher.values.first(where: { _ in true }) ?? [])
                
                /// Lastly we need to save the updated dumps to the database
                let currentlyRunningJobs: [JobRunner.JobInfo] = await Array(dependencies[singleton: .jobRunner]
                    .jobInfoFor(
                        state: .running,
                        filters: ConfigurationSyncJob.filters(
                            swarmPublicKey: swarmPublicKey,
                            whenRunning: job
                        )
                    )
                    .values)
                let updatedJob: Job? = try await dependencies[singleton: .storage].writeAsync { db in
                    /// Save the updated dumps to the database
                    try configDumps.forEach { dump in
                        try dump.upsert(db)
                        
                        Task.detached(priority: .medium) { [extensionHelper = dependencies[singleton: .extensionHelper]] in
                            extensionHelper.replicate(dump: dump)
                        }
                    }
                    
                    /// When we complete the `ConfigurationSync` job we want to immediately schedule another one with a
                    ///  `nextRunTimestamp` set to the `maxRunFrequency` value to throttle the config sync requests
                    let nextRunTimestamp: TimeInterval = (jobStartTimestamp + maxRunFrequency)
                    
                    /// If another `ConfigurationSync` job was scheduled then we can just update that one to run at `nextRunTimestamp`
                    /// and make the current job stop
                    if
                        let existingJobInfo: JobRunner.JobInfo = currentlyRunningJobs
                            .sorted(by: { lhs, rhs in lhs.nextRunTimestamp < rhs.nextRunTimestamp })
                            .first,
                        let jobId: Int64 = existingJobInfo.id,
                        let existingJob: Job = try? Job.fetchOne(db, id: jobId)
                    {
                        /// If the next job isn't currently running then delay it's start time until the `nextRunTimestamp` unless
                        /// it was manually triggered (in which case we want it to run immediately as some thread is likely waiting on
                        /// it to return)
                        let jobWasManualTrigger: Bool = (existingJob.details
                            .map { try? JSONDecoder(using: dependencies).decode(OptionalDetails.self, from: $0) }
                            .map { $0.wasManualTrigger })
                            .defaulting(to: false)
                        
                        try existingJob
                            .with(nextRunTimestamp: (jobWasManualTrigger ? 0 : nextRunTimestamp))
                            .upserted(db)
                        
                        return nil
                    }
                    
                    return try job
                        .with(nextRunTimestamp: nextRunTimestamp)
                        .upserted(db)
                }
                
                /// If we returned no `updatedJob` above then we want to stop the current job (because there is an existing job
                /// that we've already rescueduled)
                ConfigurationSyncJob.startJobsWaitingOnConfigSync(swarmPublicKey, using: dependencies)
                Log.info(.cat, "For \(swarmPublicKey) completed")
                return .success((updatedJob ?? job), stop: (updatedJob == nil))
            }
            catch {
                Log.error(.cat, "For \(swarmPublicKey) failed due to error: \(error)")
                
                /// If the failure is due to being offline then we should automatically retry if the connection is re-established
                // FIXME: Refactor this to use async/await
                let publisher2 = dependencies[cache: .libSessionNetwork].networkStatus
                    .first()
                
                let status: NetworkStatus? = await publisher2.values.first(where: { _ in true })
                switch status {
                    /// If we are currently connected then use the standard retry behaviour
                    case .connected: throw error
                        
                    /// If not then spin up a task to reschedule the config sync if we re-establish the connection and permanently
                    /// fail this job
                    default:
                        Task.detached(priority: .background) { [dependencies] in
                            // FIXME: Refactor this to use async/await
                            let publisher3 = dependencies[cache: .libSessionNetwork].networkStatus
                                .filter { $0 == .connected }
                                .first()
                            _ = await publisher3.values.first(where: { _ in true })
                            await ConfigurationSyncJob.enqueue(
                                swarmPublicKey: swarmPublicKey,
                                using: dependencies
                            )
                        }
                        
                        throw JobRunnerError.permanentFailure(error)
                }
            }
    }
    
    private static func startJobsWaitingOnConfigSync(
        _ swarmPublicKey: String,
        using dependencies: Dependencies
    ) {
        let targetJobs: [Job] = dependencies[singleton: .storage].read { db in
            return try Job
                .filter(Job.Columns.behaviour == Job.Behaviour.runOnceAfterConfigSyncIgnoringPermanentFailure)
                .filter(Job.Columns.threadId == swarmPublicKey)
                .fetchAll(db)
        }.defaulting(to: [])
        
        guard !targetJobs.isEmpty else { return }
        
        /// We use `upsert` because we want to avoid re-adding a job that happens to already be running or in the queue as these
        /// jobs should only run once
        dependencies[singleton: .storage].writeAsync { db in
            targetJobs.forEach { job in
                dependencies[singleton: .jobRunner].upsert(
                    db,
                    job: job,
                    canStartJob: true
                )
            }
        }
        Log.info(.cat, "Starting \(targetJobs.count) job(s) for \(swarmPublicKey) after successful config sync")
    }
}

// MARK: - ConfigurationSyncJob.OptionalDetails

extension ConfigurationSyncJob {
    public struct OptionalDetails: Codable {
        private enum CodingKeys: String, CodingKey {
            case wasManualTrigger
        }
        
        public let wasManualTrigger: Bool
    }
    
    /// This is additional data which can be passed to the `ConfigurationSyncJob` for a specific run but won't be persistent
    /// to disk for subsequent runs
    ///
    /// **Note:** If none of the values differ from the default then the `init` function will return `nil`
    public struct AdditionalTransientData {
        public let beforeSequenceRequests: [any ErasedPreparedRequest]
        public let afterSequenceRequests: [any ErasedPreparedRequest]
        public let requireAllBatchResponses: Bool
        public let requireAllRequestsSucceed: Bool
        
        init?(
            beforeSequenceRequests: [any ErasedPreparedRequest],
            afterSequenceRequests: [any ErasedPreparedRequest],
            requireAllBatchResponses: Bool,
            requireAllRequestsSucceed: Bool
        ) {
            guard
                !beforeSequenceRequests.isEmpty ||
                !afterSequenceRequests.isEmpty ||
                requireAllBatchResponses ||
                requireAllRequestsSucceed
            else { return nil }
            
            self.beforeSequenceRequests = beforeSequenceRequests
            self.afterSequenceRequests = afterSequenceRequests
            self.requireAllBatchResponses = requireAllBatchResponses
            self.requireAllRequestsSucceed = requireAllRequestsSucceed
        }
    }
}

// MARK: - Convenience

public extension ConfigurationSyncJob {
    static func enqueue(
        swarmPublicKey: String,
        using dependencies: Dependencies
    ) async {
        /// Upsert a config sync job if needed
        guard let job: Job = await ConfigurationSyncJob.createIfNeeded(swarmPublicKey: swarmPublicKey, using: dependencies) else {
            return
        }
        
        _ = try? await dependencies[singleton: .storage].writeAsync { db in
            dependencies[singleton: .jobRunner].upsert(db, job: job, canStartJob: true)
        }
    }
    
    @discardableResult static func createIfNeeded(
        swarmPublicKey: String,
        using dependencies: Dependencies
    ) async -> Job? {
        /// The ConfigurationSyncJob will automatically reschedule itself to run again after 3 seconds so if there is an existing
        /// job then there is no need to create another instance
        ///
        /// **Note:** Jobs with different `threadId` values can run concurrently
        guard
            await dependencies[singleton: .jobRunner]
                .jobInfoFor(
                    state: .running,
                    filters: ConfigurationSyncJob.filters(swarmPublicKey: swarmPublicKey)
                )
                .isEmpty,
            (try? await dependencies[singleton: .storage].readAsync(value: { db -> Bool in
                try Job
                    .filter(Job.Columns.variant == Job.Variant.configurationSync)
                    .filter(Job.Columns.threadId == swarmPublicKey)
                    .isEmpty(db)
            })) == true
        else { return nil }
        
        /// Otherwise create a new job
        return Job(
            variant: .configurationSync,
            behaviour: .recurring,
            threadId: swarmPublicKey
        )
    }
    
    /// Trigger the job emitting the result when completed
    ///
    /// **Note:** The `ConfigurationSyncJob` can only have a single instance running at a time, as a result this call may not
    /// resolve until after the current job has completed
    static func run(
        swarmPublicKey: String,
        beforeSequenceRequests: [any ErasedPreparedRequest] = [],
        afterSequenceRequests: [any ErasedPreparedRequest] = [],
        requireAllBatchResponses: Bool = false,
        requireAllRequestsSucceed: Bool = false,
        using dependencies: Dependencies
    ) async throws {
        guard
            let job: Job = Job(
                variant: .configurationSync,
                behaviour: .recurring,
                threadId: swarmPublicKey,
                details: OptionalDetails(wasManualTrigger: true),
                transientData: AdditionalTransientData(
                    beforeSequenceRequests: beforeSequenceRequests,
                    afterSequenceRequests: afterSequenceRequests,
                    requireAllBatchResponses: requireAllBatchResponses,
                    requireAllRequestsSucceed: requireAllRequestsSucceed
                )
            )
        else { throw JobRunnerError.missingRequiredDetails }
        
        let result: JobExecutionResult = try await ConfigurationSyncJob.run(job, using: dependencies)
        
        /// If the job was deferred it was most likely due to another `SyncPushTokens` job in progress so we should wait
        /// for the other job to finish and try again
        switch result {
            case .success: return
            case .deferred: break
        }
        
        let runningJobs: [Int64: JobRunner.JobInfo] = await dependencies[singleton: .jobRunner]
            .jobInfoFor(
                state: .running,
                filters: ConfigurationSyncJob.filters(swarmPublicKey: swarmPublicKey, whenRunning: job)
            )
        
        /// If we couldn't find a running job then fail (something else went wrong)
        guard !runningJobs.isEmpty else { throw JobRunnerError.missingRequiredDetails }
        
        let otherJobResult: JobRunner.JobResult = await dependencies[singleton: .jobRunner].awaitResult(
            forFirstJobMatching: ConfigurationSyncJob.filters(swarmPublicKey: swarmPublicKey, whenRunning: job),
            in: .running
        )
        
        /// If it gets deferred a second time then we should probably just fail - no use waiting on something
        /// that may never run (also means we can avoid another potential defer loop)
        switch otherJobResult {
            case .notFound, .deferred: throw JobRunnerError.missingRequiredDetails
            case .failed(let error, _): throw error
            case .succeeded: break
        }
    }
    
    private static func filters(swarmPublicKey: String, whenRunning job: Job? = nil) -> JobRunner.Filters {
        return JobRunner.Filters(
            include: [
                .variant(.configurationSync),
                .threadId(swarmPublicKey)
            ],
            exclude: [job?.id.map { .jobId($0) }].compactMap { $0 }   /// Exclude running job
        )
    }
}
